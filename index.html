<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" class="gr__shape2prog_csail_mit_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

              <meta name="viewport" content="width=device-width, initial-scale=1">    <link rel="shortcut icon" href="http://shape2prog.csail.mit.edu/images/favicon.ico">
        <meta name="description" content="Sequential Recommendation with Decomposed Item Feature Routing">
        <meta name="keywords" content="Sequential Recommendation with Decomposed Item Feature Routing">

        <title>Sequential Recommendation</title>
        <link rel="stylesheet" href="font.css">
        <link rel="stylesheet" href="main.css">

    </head>

    <body data-gr-c-s-loaded="true">

        <div class="outercontainer">
            <div class="container">

                <div class="content project_title">
                    <h1>Sequential Recommendation with User Evolving Preference Decomposition</h1>
                </div>

                <div class="content project_headline">
                    <center><h2>
                      <font size="3">Anonymous Author</font>&nbsp;&nbsp;
                   
                </div>


                


                <div class="content">
                    <div class="text">
                        <h1>1. Abstract</h1>
                        <p><font size=3>Modeling user sequential behaviors has recently attracted increasing attention in the recommendation domain.
Existing methods mostly assume that user preferences are coherent in the same behavior sequence.
However, user personalities are volatile and easily changed, and there can be multiple mixed preferences in user sequential behaviors.
In order to bridge this gap, in this paper, we propose to decompose user evolving preferences for building sequential recommender models.
The main building block of our idea is an item allocator, which decomposes the whole user behavior sequence into many shorter sub-sequences.
Ideally, each sub-sequence should well reflect the evolution of a thread of user preference, and different threads should be as independent as possible.
At the same time, we argue that the number of sub-sequences should be adaptively determined by considering the dynamic and evolving natures of user behaviors.
To satisfy these requirements, we design a reinforcement learning method to implement the item allocator, and generate different sub-sequences via simulating the evolution of user preferences.
More specifically, the action aims to allocate each item into a sub-sequence or create a new one according to how the previous items are decomposed.
The reward is associated with the final loss of the learning objective, encouraging to generate sub-sequences which can better fit the training data.
We conduct extensive experiments based on six real-world datasets across different domains.
Comparing with the state-of-the-art methods, empirical studies manifest that our model can on average improve the performance by about 8.21%, 12.68%, 8.69% and 4.33% on the metrics of Precision, Recall, NDCG and MRR, respectively.
For reproducing our experiments and promoting this research direction, we have released our project at https://no-one-xxx.github.io/SPLIT/.</font></p>
                    </div>
                </div>

                <div class="content">
                    <div class="text">
                        <h1>2. A Toy Example of Our Idea </h1>
                        <div class="content project_headline">
                        <div class="img" style="text-align:center">
                            <img class="img_responsive" src="intro.png" alt="Intro" style="margin:auto;max-width:80%">
                        </div>
                        <div class="text">
                            <p><font size=3>Figure 1: Examples of user inconsistent, evolving and uneven multi-thread preferences.</font></p>
                        </div>
						<div class="content">

						<h3>Contributions</h3>
                            <p><ul class="download">
							<font size=3>
                            <li> We proposed to build sequential recommender models by decomposing user evolving preferences, which facilitates more reasonable user preference representation and enhanced recommendation performance.</li>
							<li>To achieve the above idea, we design a reinforcement learning model to separate user behaviors considering their dynamic and evolving natures, where the number of sub-sequences can be adaptively determined in the optimization process. </li>
							<li>We conduct extensive experiments based on six real-world datasets to demonstrate the superiority of our model.</li>
                            </font>
                        </ul></p>
                        </div>
                    </div>
                    </div>
                </div>
                <div class="content">
                    <div class="text">
                        <h1>3. Main Results</h1>
                    </div>

                    <div class="content project_headline">
					<div class="text">
                            <p><font size=3>Table 1: Overall comparison between the baselines and our models.</font></p>
                        </div>
                        <div class="img" style="text-align:center">
                            <img class="img_responsive" src="result.png" alt="result" style="margin:auto;max-width:90%">
                        </div>
                        
                    </div>
					
                </div>
				<div class="content">
                    <div class="text">
                        <h1>4. Code and Datasets</h1>

						<h2> 4.1 Code <a href="https://drive.google.com/drive/folders/1PyftruSkQoLbPARJiDgd4SaMVeiaF4oT?usp=sharing">[link:Google Driver]</font></a></h2>
						

                        <h2>4.2 Datasets <a href="https://drive.google.com/drive/folders/1ahiLmzU7cGRPXf5qGMqtAChte2eYp9gI">[link:Google Driver]</a></font></h2>
						<div class="content project_headline">
						<div class="text">
                            <p><font size=3>Table 3: Statistics of the datasets used in our experiments.</font></p>
                        </div>
                        <div class="img" style="text-align:center">
                            <img class="img_responsive" src="data.png" alt="result" style="margin:auto;max-width:90%">
                        </div>
                        
                        </div>
                    </div>
                </div>

               
                <div class="content">
                    <div class="text">
                        <h1>5. Detailed Parameter Search Ranges</h1>
						<ul class="download">
						<font size=3>
                            <li><p>Learning rate: [0.01,0.005,0.001]</p></li>
							<li><p>train_batch_size:[256,512,1024]</p></li>
							<li><p>threshold_hyperparameter:[0.,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]</p></li>
							<li><p>w1:[−0.1,−0.2,−0.3,−0.4,−0.5]</p></li>
							<li><p>embedding/hidden size:[64,128,256]</p></li>
							<li><p>w2:[0.2, 0.4, 0.6]</p></li>
						</font>
                        </ul>
						
                    </div>
                </div>


                

<div id="download_plus_animation"></div></body></html>


