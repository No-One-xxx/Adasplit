<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" class="gr__shape2prog_csail_mit_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

              <meta name="viewport" content="width=device-width, initial-scale=1">    <link rel="shortcut icon" href="http://shape2prog.csail.mit.edu/images/favicon.ico">
        <meta name="description" content="Sequential Recommendation with Decomposed Item Feature Routing">
        <meta name="keywords" content="Sequential Recommendation with Decomposed Item Feature Routing">

        <title>Sequential Recommendation</title>
        <link rel="stylesheet" href="font.css">
        <link rel="stylesheet" href="main.css">

    </head>

    <body data-gr-c-s-loaded="true">

        <div class="outercontainer">
            <div class="container">

                <div class="content project_title">
                    <h1>Sequential Recommendation with Adaptive Preference Disentanglement</h1>
                </div>

                <div class="content project_headline">
                    <center><h2>
                      <font size="3">Anonymous Author</font>&nbsp;&nbsp;
                   
                </div>


                


                <div class="content">
                    <div class="text">
                        <h1>1. Abstract</h1>
                        <p><font size=3>Sequential recommendation holds the promise of being able to infer user preference from the history information.
Existing methods mostly assume coherent user preference in the history information, and deploy a unified model to predict the next behavior.
However, user preferences are naturally diverse, and different users may enjoy their own personalities, which makes the history information mixed of heterogeneous user preferences.
Inspired by this practical consideration, in this paper, we proposed a novel sequential recommender model by disentangling different user preferences.
The main building block of our idea is a behavior allocator, which determines how many sub-sequences the history information should be decomposed into, and how to allocate each item into these sub-sequences.
In particular, we regard the disentanglement of user preference as a Markov decision process, and design a reinforcement learning method to implement the behavior allocator.
The reward in our model is designed to assign the target item to the nearest sub-sequence, and simultaneously encourage orthogonality between the generated sub-sequences.
To make the disentangled sub-sequences not too sparse, we introduce a curriculum reward, which adaptively penalizes the action of creating a new sub-sequence.
We conduct extensive experiments based on real-world datasets, and compare with many state-of-the-art models to verify the effectiveness of our model.
Empirical studies manifest that our model can on average improve the performance by about 7.42$\%$ and 11.98$\%$ on metrics NDCG and MRR, respectively.
For reproducing our experiments and promoting this research direction, we have released our project at https://no-one-xxx.github.io/Adasplit/.</font></p>
                    </div>
                </div>

                <div class="content">
                    <div class="text">
                        <h1>2. A Toy Example of Our Idea </h1>
                        <div class="content project_headline">
                        <div class="img" style="text-align:center">
                            <img class="img_responsive" src="intro.png" alt="Intro" style="margin:auto;max-width:80%">
                        </div>
                        <div class="text">
                            <p><font size=3>Figure 1: Illustration of user diverse preferences, where different users may have various personalities, and thus has different number of interests in their behavior sequence.</font></p>
                        </div>
						<div class="content">

						<h3>Contributions</h3>
                            <p><ul class="download">
							<font size=3>
                            <li> We proposed to build sequential recommender models by adaptively disentangling user preferences, which, to the best of our knowledge, is the first time in the recommendation domain.</li>
							<li>To achieve the above idea, we design a reinforcement learning (RL) model to allocate user behaviors and adaptively create new sub-sequences, which captures the evolving nature of user preference. </li>
							<li>We conduct extensive experiments on four datasets with several benchmarks to demonstrate the effectiveness of our model and for promoting this research direction, we have released our project at https://no-one-xxx.github.io/ Adasplit/.</li>
                            </font>
                        </ul></p>
                        </div>
                    </div>
                    </div>
                </div>
                <div class="content">
                    <div class="text">
                        <h1>3. Main Results</h1>
                    </div>

                    <div class="content project_headline">
					<div class="text">
                            <p><font size=3>Table 1: Overall comparison between the baselines and our models.</font></p>
                        </div>
                        <div class="img" style="text-align:center">
                            <img class="img_responsive" src="result.png" alt="result" style="margin:auto;max-width:90%">
                        </div>
                        
                    </div>
					
                </div>
				<div class="content">
                    <div class="text">
                        <h1>4. Code and Datasets</h1>

						<h2> 4.1 Code <a href="https://drive.google.com/drive/folders/1PyftruSkQoLbPARJiDgd4SaMVeiaF4oT?usp=sharing">[link:Google Driver]</font></a></h2>
						

                        <h2>4.2 Datasets <a href="https://drive.google.com/drive/folders/1ahiLmzU7cGRPXf5qGMqtAChte2eYp9gI">[link:Google Driver]</a></font></h2>
						<div class="content project_headline">
						<div class="text">
                            <p><font size=3>Table 3: Statistics of the datasets used in our experiments.</font></p>
                        </div>
                        <div class="img" style="text-align:center">
                            <img class="img_responsive" src="data.png" alt="result" style="margin:auto;max-width:90%">
                        </div>
                        
                        </div>
                    </div>
                </div>

               
                <div class="content">
                    <div class="text">
                        <h1>5. Detailed Parameter Search Ranges</h1>
						<ul class="download">
						<font size=3>
                            <li><p>Learning rate: [0.1,0.01,0.001,0.0001]</p></li>
							<li><p>Thresholds:[0.2,0.3,0.4,0.5,0.6,0.7,0.8]</p></li>
							<li><p>Inner size:[32,64,128,256]</p></li>
							<li><p>num_layers:[1,2,3]</p></li>
							<li><p>Hidden size:[32,64,128,256]</p></li>
						</font>
                        </ul>
						
                    </div>
                </div>


                <div class="content">
                    <div class="text">
                        <h1>6. Runtime Environment</h1>
						<ul class="download">
						<font size=3>
                            <li><p>System:Linux dell-PowerEdge-R730</p></li>
							<li><p>CPU: Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz</p></li>
							<li><p>CPU-Memory:25G</p></li>
							<li><p>GPU:NVIDIA Corporation GV100 [TITAN V] (rev a1)</p></li>
                            <li><p>GPU-Memory:12G</p></li>
							<li><p>Pytorch: 1.7.0</p></li>
							<li><p>CUDA:10.1</p></li>
						</font>
                        </ul>
						
                    </div>
                </div>

<div id="download_plus_animation"></div></body></html>


